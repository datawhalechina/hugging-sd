# 6-1-1文生视频原理

- [0 章节目标](#0-章节目标)
- [1 CogVideo预训练模型 ](#1-CogVideo预训练模型)
  - [1.1 简介](#11-简介)
  - [1.2 多帧率分层训练](#12-多帧率分层训练)
  - [1.3 双通道注意力机制](#13-双通道注意力机制)
  - [1.4 自回归生成中窗口注意力的转移](#14-自回归生成中窗口注意力的转移)

## 0 章节目标

- 了解文生视频模型
- 了解文生视频模型的技术

## 1 CogVideo预训练模型

### 1.1 简介

文生视频模型由于可训练的数据少，且视频持续时间差别很大，使用固定帧数的剪辑训练会破坏动作的连贯性，使模型混淆动作含义。

在这篇文章中，作者使用了多帧率分层训练法，提出了一个大规模的文生视频的预训练模型。(https://arxiv.org/abs/2205.15868)

### 1.2 多帧率分层训练

![多帧率分层训练](/content/images/6-1-1_1multi_frame.png)

其原理可以简单理解为：

1、先根据语义生成关键视频帧的图像

2、基于关键帧的图像，生成连贯动作的补帧

也就是说，模型有两个部分需要训练，一个顺序的生成模型和一个递归的插值模型

由于帧的插值依赖于双向信息，所以模型应该同时有单向和双向区域。图中阶段一的所有帧和阶段二的2、4帧为单向区域，阶段二的其余帧属于双向区域。

### 1.3 双通道注意力机制

![双通道注意力机制](/content/images/6-1-1_2dual_channel_attention.png)



CogVideo模型使用了CogView2文生图预训练模型的初始参数，这张图展示了其transformer layer层的结构，只有初始的参数是继承自CogView2，而后只有attention plus部分可训练。

包含Sandwich-LN的dual-channel注意力模块可计算为：


$$
\begin{aligned}
\widetilde{x}& =\alpha\cdot\mathrm{attention-base}(\mathrm{LayerNorm}(x_{in}))  \\
&+(1-\alpha)\cdot\text{attention-plus}(\text{LayerNorm}(x_{in})), \\
x_{out}& =x_{in}+\text{Layer}\mathrm{Norm}(\widetilde{x}). 
\end{aligned}
$$

在3D局部注意力中，对于位置 $(t,x,y)$ 的标记，其感受野（RF）是一个具有范围 $l_t,l_x,l_y\in\mathbb{N}^+$ 的三维块，其中 $(t,x,y)$ 对应于时间、高度和宽度的坐标。

$$
\begin{aligned}
\mathrm{RF}_{(t,x,y)} =\{(k,i,j)\:|\:|x-i|<l_{x}, |y-j|<l_{y}, |t-k|<l_{t}, \:(k,i,j)\notin\mathrm{Mask}_{(t,x,y)}\}
\end{aligned}
$$

在这个连续生成模型的第一阶段，Mask $_{(t,x,y)}$ 表示对标记 $(t,x,y)$ 的注意力掩码。这个掩码确保了自回归顺序。在插值模型的第二阶段，该掩码被设计成与 CogLM 中一样，以便使所有帧都能看到已知的帧。

### 1.4 自回归生成中窗口注意力的转移

为了进一步缓解训练和推理过程中时间通道中的大时间和记忆开销，参考Swin注意。

![Swin Transformer](/content/images/6-1-1_3swin.png)

Swin Transformer 是通过将 Transformer 模块中的标准多头自注意力（MSA）模块替换为基于移位窗口的模块而构建的，其他层保持不变。 如图 3(b) 所示，Swin Transformer 模块由基于移位窗口的 MSA 模块组成，后跟中间带有 GELU 非线性的 2 层 MLP。 在每个 MSA 模块和每个 MLP 之前应用 LayerNorm (LN) 层，并在每个模块之后应用残差连接。

原始的Swin注意只应用于非自回归场景，通过在移位窗口中应用自回归注意掩模，将其扩展到自回归和时间场景。Swin注意力提供了在不同帧的遥远区域中并行生成的机会，这进一步加速了自回归生成。

![3D 自回归swin](/content/images/6-1-1_43Dswin.png)



在3D自回归swin 注意力（以窗口大小 2 × 2 为例）中，红色框中的标记只能关注（直接或间接）黄色或绿色标记。 第i帧中的灰色令牌和红色框中的令牌可以并行生成。

如图所示，可以在完成前面所有帧的生成之前开始生成后面帧中的部分令牌 - 它们可以并行工作。 假设X，Y是每帧的高度和宽度，Ax，Ay是移动窗口的高度和宽度。 对于 (t1, x1, y1) 和 (t2, x2, y2) 处的两个标记，t1 < t2，如果满足以下条件，则后者不能直接或间接关注前者：

$$
(x_1-x_2)Y+(y_1-y_2)\ge(t_2-t_1+1)(A_xY+A_y),
$$

这意味着在第 $t$ 帧中，第 $i$ 个标记可以与前一帧中的第 $(i-A_xY-A_y)$ 个标记一起以最多 $\lfloor\frac{XY}{A_{x}Y+A_{y}}\rfloor$ 个标记的并行方式在第 $(t+1)$ 帧中生成。通过这种方式，与只能逐个生成标记的自回归标准注意相比，可以极大地提高并行性并加速推断过程。
