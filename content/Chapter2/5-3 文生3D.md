# 5-3 文生3D
(目录)

# 0 章节目标
- 理解Dreamfusion、zero123原理，理解文生3D原理
- 会运用文生3D的能力到自己项目中

# 1 显式生成方法
显示生成方法中有[3-1](./3-1%20背景及应用.md)中所介绍的[point-e](https://github.com/openai/point-e)、[shape-e](https://github.com/openai/shap-e)等生成点云的工作。


<table>
    <tbody>
        <tr>
            <td align="center">
                <img src="https://github.com/openai/shap-e/blob/main/samples/a_chair_that_looks_like_an_avocado/2.gif" alt="A chair that looks like an avocado">
            </td>
            <td align="center">
                <img src="https://github.com/openai/shap-e/blob/main/samples/an_airplane_that_looks_like_a_banana/3.gif" alt="An airplane that looks like a banana">
            </td align="center">
            <td align="center">
                <img src="https://github.com/openai/shap-e/blob/main/samples/a_spaceship/0.gif" alt="A spaceship">
            </td>
        </tr>
        <tr>
            <td align="center">A chair that looks<br>like an avocado</td>
            <td align="center">An airplane that looks<br>like a banana</td>
            <td align="center">A spaceship</td>
        </tr>
        <tr>
            <td align="center">
                <img src="https://github.com/openai/shap-e/blob/main/samples/a_birthday_cupcake/3.gif" alt="A birthday cupcake">
            </td>
            <td align="center">
                <img src="https://github.com/openai/shap-e/blob/main/samples/a_chair_that_looks_like_a_tree/2.gif" alt="A chair that looks like a tree">
            </td>
            <td align="center">
                <img src="https://github.com/openai/shap-e/blob/main/samples/a_green_boot/3.gif" alt="A green boot">
            </td>
        </tr>
        <tr>
            <td align="center">A birthday cupcake</td>
            <td align="center">A chair that looks<br>like a tree</td>
            <td align="center">A green boot</td>
        </tr>
        <tr>
            <td align="center">
                <img src="https://github.com/openai/shap-e/blob/main/samples/a_penguin/1.gif" alt="A penguin">
            </td>
            <td align="center">
                <img src="https://github.com/openai/shap-e/blob/main/samples/ube_ice_cream_cone/3.gif" alt="Ube ice cream cone">
            </td>
            <td align="center">
                <img src="https://github.com/openai/shap-e/blob/main/samples/a_bowl_of_vegetables/2.gif" alt="A bowl of vegetables">
            </td>
        </tr>
        <tr>
            <td align="center">A penguin</td>
            <td align="center">Ube ice cream cone</td>
            <td align="center">A bowl of vegetables</td>
        </tr>
    </tbody>
</table>


这种方式生成的3D物体，易于修改，但是可以看到渲染质量并不好。当隐式的表征特别是dreamfusion横空出世之后，3d生成方面的许多工作都围绕它来展开了。


# 2 隐式生成方法
在[3-2 NeRF神经辐射场](./3-2%20NeRF神经辐射场.ipynb)章节我们详细介绍了nerf的原理以及使用，在此基础上，我们将在这个章节介绍一下分别通过隐式生成方法和显式生成方法来带大家了解3D生成的主流方法，以便于大家能快速入门并掌握基本的用法，同时也以此了解行业发展方向。

首先我们了解隐式生成方法的生成方法，在[3-2 NeRF神经辐射场](./3-2%20NeRF神经辐射场.ipynb)中我们有详细介绍五种目前主流的三维表征方法，后两种即[SDF](https://en.wikipedia.org/wiki/Signed_distance_function)和[NeRF](https://en.wikipedia.org/wiki/Neural_radiance_field)是隐式的表征方法，显然隐式生成方法的生成方法就是利用sdf和nerf的表征来生成三维的物体和场景。本章节将主要讨论的是基于nerf的隐式生成方法，基于sdf的会简单介绍。


## 2-1 DreamFusion
首先，dreamfusion论文来自[DreamFusion: Text-to-3D using 2D Diffusion](https://arxiv.org/abs/2209.14988)，看下效果。

"a raccoon astronaut holding his helmet"

<img alt="dreamfusion-demo" height="200" src="./images/3-3_1-dreamfusion-demo.gif" width="300"/>

跟显式的表征方法对比，可以看到质量有明显的提升，但是质量还远未到真正可以用的地步。下面我们大致讲一下Dreamfusion的原理。

<img alt="dreamfusion-framework" height="300" src="./images/3-3_2-deamfusion-framework.png" width="700"/>

- 首先，通过2D的文生图大模型（这里采用的是[Imagen](https://imagen.research.google/)）得到图片a
- 同时，在空间中随机初始化一个NeRF，然后NeRF选择一个视角pose并rendering得到一张图片b，注意图片的输出与Imagen输出的图片分辨率保持一致
- 然后，将图片叠加随机噪声放入大模型中进行去噪过程并得到a'。DM的去噪过程实际上是在估计噪声，然后估计的噪声和添加的噪声之间求loss。所以a'与加入的高斯噪声相减，便有了c'。
c'将直接用来更新NeRF的权重，从而使render的结果b更加真实，如此迭代下，最终便可以得到text-to-3D的效果。


实质上NeRF的作用可以看作是一个大的约束器，去监督生成模型生成符合这个约束器的一系列连续视角的图片。这篇工作是可以算是文生3D方向迈出的关键一步，直接奠定了NeRF类方法在3D生成方面的应用

更多细节请参考：[DreamFusion: text-to-3D using 2D diffusion](https://www.zhihu.com/search?type=content&q=dreamfusion)


(Todo 代码演示)

## 2-2 Zero123
首先，Zero123论文来自[Zero-1-to-3: Zero-shot One Image to 3D Object](https://arxiv.org/abs/2303.11328)，看下效果。

<img alt="zero123-framework" height="300" src="./images/3-3_3-zero123.png" width="700"/>

这种方法输入将一张RGB的图片和一个pose(视角)输入一个已经在sd上微调过的大模型中，然后模型根据输入输出一个关于这张图对应视角的图像。
<img alt="zero123-generation" height="300" src="./images/3-3_4-zero123.png" width="500"/>

通过这样的方法，可以生成很多不同视角的图像。然后再放入[NeRF](./3-2%20NeRF神经辐射场.ipynb)类的方法中进行训练得到最终的辐射场。
<img alt="dreamfusion-nerf" height="300" src="./images/3-3_5-zero123.png" width="500"/>

可以通过[huggingface-zero123](https://huggingface.co/spaces/cvlab/zero123-live)、[zero123](https://zero123.cs.columbia.edu/)亲自动手玩一玩，直观的感受下。
注意，由于目前很多方法都有后文所提到的缺点[3 缺点与改进]()，所以为了能生成更好更高质量的内容，需要注意以下几点：

- 尽量保持图片清晰且背景为白或单一
- 尽量保证图片中仅包含一个物体，可以采用iphone抠图操作
- 物体尺寸不易太大，物体在图片中占比适中即可


## 2-3 ProlificDreamer
生成质量一直是三维生成中一项非常重要的指标，近期发表在NeurIPS的[ProlificDreamer](https://ml.cs.tsinghua.edu.cn/prolificdreamer/)由于其惊艳的生成效果而收到广泛关注。

"Michelangelo style statue of dog reading news on a cellphone."

<img alt="dreamfusion-demo" height="500" src="./images/3-3_7-prolificdreamer.gif" width="600"/>

整体的方法结构类似dreamfusion，但是有些修改。他们的方法能够训练更高分辨率（512*512）的物体，能够展现更高的纹理细节。


# 3 缺点与改进

## 3.1 生成质量
生成质量的问题主要表现在两个方面：

- 对于单个物体而言
  - 生成视角一致性问题，即从不同视角看该物体都能认为它是同一个问题
  - 生成分辨率问题，即使目前生成分辨率最高的模型，依然难以将结果直接利用
  - 生成可控性问题，目前还没有比较有效能够控制3d结构的约束器类似与2d中的[ControlNet](https://github.com/lllyasviel/ControlNet)

- 对于场景而言
  - 目前还没有做到直接text2scene的模型
  - 需要解决物体与物体之间的合理性问题
  - 分辨率问题，生成时间等等

目前主流的文生3D方法对于3D物体的表征几乎都是基于NeRF，同时它也是未来这个领域最具潜力的一种表征方式。 所以从NeRF出发去提升它对于三维世界的表征能力和速度则显得尤为关键，目前在质量方面也有一系列提升的方法，比如
[MipNeRF](https://jonbarron.info/mipnerf/),[MipNeRF360](https://jonbarron.info/mipnerf360/),[TensorRF](https://apchenstu.github.io/TensoRF/),[ZipNeRF](https://jonbarron.info/zipnerf/),[Tri-MipRF](https://arxiv.org/abs/2307.11335)等都取得了非常显著的效果提升；同时在提升速度方面也有非常多改进工作[Plenoxels](),[Instant-ngp]()等。

当然也有从生成模型出发，去设计一个更利于3d生成和表征的模型结构或损失函数，比如[ProlificDreamer](https://ml.cs.tsinghua.edu.cn/prolificdreamer/)等

## 3.2 生成时间
生成时间的问题主要表现在两个方面：

- 模型训练时间
  - 模型训练需要的时间较长
  - 如何更快速高效得提取模型中的有效信息

- 模型推理时间
  - NeRF本身训练和渲染就很花时间，可以寻找更高效的表征方式

目前的主流方法都是NeRF+2D预训练大模型，所以如何更高效地从2D大模型中去提炼一个3D的结构就显得尤为关键，并且去寻找一种更利于生成的一种3D表征方式也更为重要。
近期，有许多新的方法去对NeRF原本的数据表征和存储方法做了改进，并且使用到生成中校效果不错。

比如，[EG3D](https://nvlabs.github.io/eg3d/)通过StyleGAN2+TensoRF实现3d的人脸合成。

<img alt="dreamfusion-demo" height="500" src="./images/3-3_8-EG3D.gif" width="500"/>

比如，一种非常火热的表征方式[3D Gaussian Splatting for Real-Time Radiance Field Rendering](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)，有人就通过3D Gaussian Splatting+2d diffusion实现了快速高质量的3d物体合成[dreamgaussian](https://github.com/dreamgaussian/dreamgaussian)。

<img alt="dreamfusion-demo" height="400" src="./images/3-3_9-gaussiandreamer.gif" width="600"/>









